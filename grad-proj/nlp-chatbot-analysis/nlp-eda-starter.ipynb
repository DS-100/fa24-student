{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ecdfe2-d0a4-4458-9498-11370a5f155d",
   "metadata": {},
   "source": [
    "# EDA of Chatbot Arena Dataset -- Starter Notebook\n",
    "\n",
    "This notebook aims to help you explore the Chatbot Arena dataset, where two chatbots answer human questions, and users vote on the best response. Through this EDA, we will:\n",
    "- Understand the dataset structure and contents.\n",
    "- Explore the distribution of questions, responses, and chatbots.\n",
    "- Identify patterns in the data to guide future modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e682f63a-4115-4e2c-90ba-5c1f0eef35c3",
   "metadata": {},
   "source": [
    "## 1. Understanding the Dataset\n",
    "The source dataset comes from https://huggingface.co/datasets/lmsys/chatbot_arena_conversations. The author describes the dataset as follows:\n",
    "\n",
    "> This dataset contains 33K cleaned conversations with pairwise human preferences. It is collected from 13K unique IP addresses on the Chatbot Arena from April to June 2023. Each sample includes a question ID, two model names, their full conversation text in OpenAI API JSON format, the user vote, the anonymized user ID, the detected language tag, the OpenAI moderation API tag, the additional toxic tag, and the timestamp.\n",
    "\n",
    "[Chatbot Arena](https://chat.lmsys.org/) is a platform where users can ask questions and two chatbots will provide answers. The user then votes on which chatbot provided the best answer. The dataset contains the questions, the answers, and the user votes.\n",
    "\n",
    "### Main Dataset\n",
    "\n",
    "- **Conversation Data**\n",
    "You can find the processed dataset in `./chatbot-arena-conversations.jsonl.gz`. The dataset is in JSON line format and compressed using gzip. It has gone through the following preprocessing steps to make analysis easier:\n",
    "\n",
    "    - Removed non-English conversations.\n",
    "    - Removed conversations with more than one round.\n",
    "    - Removed conversations classified as toxic or harmful.\n",
    "\n",
    "The dataset you will be working with contains `25282` rows (out of `33000` total rows) and `7` columns. The columns are:\n",
    "\n",
    "- `question_id`: A unique identifier for the question.\n",
    "- `model_a`: The name of the first chatbot model.\n",
    "- `model_b`: The name of the second chatbot model.\n",
    "- `winner`: The name of the chatbot model that won the user vote.\n",
    "- `judge`: The anonymized user ID that voted.\n",
    "- `conversation_a`: The conversation between the user and `model_a`.\n",
    "- `conversation_b`: The conversation between the user and `model_b`.\n",
    "\n",
    "An example of the data is as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"question_id\": \"c0fc42c6f5f14f2aa5a89f71f8553730\",\n",
    "  \"model_a\": \"vicuna-13b\",\n",
    "  \"model_b\": \"koala-13b\",\n",
    "  \"winner\": \"model_a\",\n",
    "  \"judge\": \"arena_user_973\",\n",
    "  \"conversation_a\": [\n",
    "    {\n",
    "      \"content\": \"Count from 1 to 10 with step = 3\",\n",
    "      \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"1, 4, 7, 10\\n\\nCounting with a step of 3 means that you will increase the number by 3 each time. So, the first number is 1, the next is 4 (1 + 3), the next is 7 (4 + 3), and so on.\",\n",
    "      \"role\": \"assistant\"\n",
    "    }\n",
    "  ],\n",
    "  \"conversation_b\": [\n",
    "    {\n",
    "      \"content\": \"Count from 1 to 10 with step = 3\",\n",
    "      \"role\": \"user\"\n",
    "    },\n",
    "    {\n",
    "      \"content\": \"1, 4, 7, 10\",\n",
    "      \"role\": \"assistant\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Auxiliary Datasets\n",
    "\n",
    "There are four auxiliary datasets that you can use to help with your analysis:\n",
    "\n",
    "- **Embedding Data**\n",
    "    - `./chatbot-arena-prompts-embeddings.npy` contains the 256-dimensional text embeddings for each of the human questions. The embeddings are generated using OpenAI’s text-embedding model. We will explain what embeddings are and how you can use them later.\n",
    "    - `./chatbot-arena-model_a_response-embeddings.npy` contains the 256-dimensional text embeddings for each of the model a responses (second half of conversation a). The embeddings are generated using OpenAI’s text-embedding model. \n",
    "     - `./chatbot-arena-model_b_response-embeddings.npy` contains the 256-dimensional text embeddings for each of of the model b responses (second half of conversation b). The embeddings are generated using OpenAI’s text-embedding model.\n",
    "\n",
    "- **Topic Modeling and Hardness Score Data**\n",
    "- `./chatbot-arena-gpt3-scores.jsonl.gz` ([example row](https://gist.github.com/simon-mo/25c5d532bccc7f28b404cffdfe719e6e#file-example-aux-row-json))\n",
    " contains labels for the dataset, which you can use for later modeling tasks. It has the following fields:\n",
    "  - **question_id**: The unique identifier for the question, as seen in `./chatbot-arena-conversations.jsonl.gz`.\n",
    "  - **prompt**: The extracted human question. This is equivalent to the first message in `conversation_a` and `conversation_b` in `./chatbot-arena-conversations.jsonl.gz`.\n",
    "  - **openai_scores_raw_choices_nested**: The response from OpenAI GPT 3.5 model. It contains:\n",
    "    - The evaluated topic model\n",
    "    - The reason for a hardness score (from 1 to 10)\n",
    "    - The score value\n",
    "\n",
    "  For each prompt, we have 3 responses from GPT 3.5 because it is a probabilistic model. In the real world, multiple annotators may provide different labels for ground truth data. We extracted the following fields into columns:\n",
    "\n",
    "  - **topic_modeling_1, topic_modeling_2, topic_modeling_3**: Topic modeling for the first, second, and third response. Each topic contains two words.\n",
    "  - **score_reason_1, score_reason_2, score_reason_3**: The reasons for the hardness score for the first, second, and third response.\n",
    "  - **score_value_1, score_value_2, score_value_3**: The hardness score for the first, second, and third response.\n",
    " \n",
    "```json\n",
    "{\n",
    "  \"question_id\": \"58210e39b3fd4441a2bd4a518bb44c2d\",\n",
    "  \"prompt\": \"What is the difference between OpenCL and CUDA?\",\n",
    "  \"openai_scores_raw_choices_nested\": [\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 0,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Technical Comparison\\\",\\n    \\\"score_reason\\\": \\\"This prompt requires the AI to accurately compare and contrast two distinct technologies, OpenCL and CUDA. It assesses the AI's factual accuracy and knowledge of these technologies, as well as its ability to articulate the differences between them.\\\",\\n    \\\"score_value\\\": 9\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 1,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Software Comparison\\\",\\n    \\\"score_reason\\\": \\\"This prompt assesses the AI's factual accuracy in distinguishing between two similar but distinct software frameworks.\\\",\\n    \\\"score_value\\\": 8\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"finish_reason\": \"stop\",\n",
    "      \"index\": 2,\n",
    "      \"logprobs\": null,\n",
    "      \"message\": {\n",
    "        \"content\": \"{\\n    \\\"topic_modeling\\\": \\\"Comparison, Technology\\\",\\n    \\\"score_reason\\\": \\\"This prompt requires the AI to demonstrate knowledge of two different technologies, compare their features, and explain their distinctions. This task assesses the AI's factual accuracy and proficiency in understanding complex technological concepts.\\\",\\n    \\\"score_value\\\": 9\\n}\",\n",
    "        \"role\": \"assistant\",\n",
    "        \"function_call\": null,\n",
    "        \"tool_calls\": null\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"topic_modeling_1\": \"Technical Comparison\",\n",
    "  \"score_reason_1\": \"This prompt requires the AI to accurately compare and contrast two distinct technologies, OpenCL and CUDA. It assesses the AI's factual accuracy and knowledge of these technologies, as well as its ability to articulate the differences between them.\",\n",
    "  \"score_value_1\": 9,\n",
    "  \"topic_modeling_2\": \"Software Comparison\",\n",
    "  \"score_reason_2\": \"This prompt assesses the AI's factual accuracy in distinguishing between two similar but distinct software frameworks.\",\n",
    "  \"score_value_2\": 8,\n",
    "  \"topic_modeling_3\": \"Comparison, Technology\",\n",
    "  \"score_reason_3\": \"This prompt requires the AI to demonstrate knowledge of two different technologies, compare their features, and explain their distinctions. This task assesses the AI's factual accuracy and proficiency in understanding complex technological concepts.\",\n",
    "  \"score_value_3\": 9\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d5d37-3635-4412-bd0e-9f28874116ef",
   "metadata": {},
   "source": [
    "## 2. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594a806-ebcc-4947-bf4b-938cdbba27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f370f-b361-453d-8aa3-85e1dc3555d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Dataset\n",
    "\n",
    "# Conversation Data -- we will use this data in the \"Conversation Data\" section\n",
    "df = pd.read_json(\n",
    "    \"/home/jovyan/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-conversations.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789575c5-9d08-4f85-bf4b-1622b84e23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Datasets\n",
    "\n",
    "# Embedding Data -- we will use this data in the \"Embedding Data\" section\n",
    "prompt_embeddings = np.load(\n",
    "    \"/home/jovyan/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-prompts-embeddings.npy\"\n",
    ")\n",
    "\n",
    "response_a_embeddings = np.load(\n",
    "    \"/home/jovyan/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-model_a_response-embeddings.npy\"\n",
    ")\n",
    "\n",
    "response_b_embeddings = np.load(\n",
    "    \"/home/jovyan/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-model_b_response-embeddings.npy\"\n",
    ")\n",
    "\n",
    "# Topic Modeling and Hardness Score Data -- we will use this data in the \"Topic Modeling and Hardness Score Data\" section\n",
    "topic_and_hardness = pd.read_json(\n",
    "    \"/home/jovyan/data100-shared-readwrite/fa24_grad_project_data/nlp-chatbot-analysis_data/training-set/chatbot-arena-gpt3-scores.jsonl.gz\",\n",
    "    lines=True,\n",
    "    compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79dea3a-89c9-458c-a36e-e322c4c5fe20",
   "metadata": {},
   "source": [
    "## 3. Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8786dd59-23e1-44b6-82ce-b5aaf4efbba4",
   "metadata": {},
   "source": [
    "### Converstation Data\n",
    "\n",
    "Let's investigate the conversation data first (`chatbot-arena-conversations.jsonl.gz`). It is in JSON line format, compressed with `gzip`. You can load the data with `pd.read_json`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ed851-e2a8-4b2a-94fa-2e03195759a6",
   "metadata": {},
   "source": [
    "Before diving into any analysis, it's important to understand the structure of the dataset. In this section, we'll check the basic details of the data, such as the number of rows, column names, data types, and any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcd55c-8332-4622-9336-129aa22215f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff52110-9e14-43a3-aa80-716240937942",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fe89b-5400-4c99-8fe8-c007f7dd52ca",
   "metadata": {},
   "source": [
    "As an example to guide you through the exploratory process, let’s investigate the length distribution of the prompt. This helps answer questions like (this is related to the ``Distribution of the prompt and response length'' requirement of the EDA assingment).\n",
    ": \n",
    "\n",
    "- Do the arena users ask long or short questions?\n",
    "- What is the average length of prompts that users give to the chatbots?\n",
    "\n",
    "By analyzing the length of the prompts, you can start forming hypotheses about how the length might affect model performance or user votes. This example will guide you in asking similar questions about other aspects of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8d23d-158d-41f7-8942-f0f301740340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prompt\"] = df[\"conversation_a\"].str[0].str[\"content\"]\n",
    "df[\"prompt\"].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd2e311-9ef9-4b3c-9cee-4a5b73c00743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"prompt_length\"] = df[\"prompt\"].str.len()\n",
    "df[\"prompt_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0029eb-d56a-4365-b2a8-6ad7f25ac241",
   "metadata": {},
   "source": [
    "Looks like the mean length of the prompt is about **200 characters, while the median is 72 characters**. This suggests that the distribution is **right-skewed!** Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0656e01-e3db-4673-bdd1-c47446ca96f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the length of the prompt\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df[\"prompt_length\"], kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f738b9-3b65-4eee-b550-73aec71d58c3",
   "metadata": {},
   "source": [
    "Now, can you apply the same thought process to visualize the distribution of the response length? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e175e-a2be-4529-8c86-00c7a8b628f2",
   "metadata": {},
   "source": [
    "Applying the same logic you used to make the `prompt` column, you should also make the columns `model_a_response` and `model_b_response` by extracting the **second half** of the content from `conversation_a` and `conversation_b` respectively in order to look into specific content of the model responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626074ea-6660-4cfd-a4fa-17d2249d9304",
   "metadata": {},
   "source": [
    "### Embedding Data\n",
    "\n",
    "Text embedding models transform natural language text into numerical vectors. The vectors are generated in such a way that semantically similar text are close to each other in the vector space. In the real world, these embeddings to find similar questions or to cluster the questions.\n",
    "\n",
    "Concretely, the auxiliary dataset,`./chatbot-arena-prompts-embeddings.npy`, `./chatbot-arena-model_a_response-embeddings.npy`, and `./chatbot-arena-model_b_response-embeddings.npy`, contains 256-dimensional text embeddings for each of the human questions, model a responses and model b responses respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a4bbd-4d47-4c45-b035-34425934fc4b",
   "metadata": {},
   "source": [
    "In this section, we will walk you through an example of computing the similarity between prompts using the precomputed embeddings (`./chatbot-arena-prompts-embeddings.npy`). The goal is to find prompts that are most similar to a given prompt based on their embeddings.\n",
    "\n",
    "Before we get started, let's first load and output the `propmt_embeddings` which we have loaded in the second section to see what they look like. Each embedding is a 256-dimensional vector that represents the semantic meaning of a prompt. These vectors allow us to compare prompts based on their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2dfdba-9cca-4bf0-8f5c-f21b7a59854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44759049-09d6-4850-b21d-c194943a9c92",
   "metadata": {},
   "source": [
    "The embeddings are a matrix where each row corresponds to a prompt from the dataset. Each row is a 256-dimensional vector that captures the semantic meaning of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d2105-bbf0-4323-a0bf-603d0ce70f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa079ce-05af-498f-b87a-d4f3720ae5e5",
   "metadata": {},
   "source": [
    "The embeddings array has a shape of `(25282, 256)`, meaning there are 25,282 embeddings (one for each prompt), and each embedding is a 256-dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7543f-a340-4ee0-af64-956ad83e2254",
   "metadata": {},
   "source": [
    "Next, we will:\n",
    "1. Take a sample of the embeddings to reduce computation time.\n",
    "2. Compute the dot product between the embeddings to measure similarity.\n",
    "3. Retrieve the most similar prompts to a chosen source prompt.\n",
    "4. Output the top 5 similar prompts.\n",
    "(This is related to the ``(Open-ended) Explore the prompt topics in the dataset (topic modeling).'' requirement of the EDA assignment)\n",
    "\n",
    "Start by taking a sample of the embeddings and calculating the similarity between them using the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127aa9f-763e-4086-9ae5-9464ff37f3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to find the closest prompt to a given prompt\n",
    "embeddings_sample = prompt_embeddings[:1000]\n",
    "\n",
    "# Compute the dot product between the embeddings\n",
    "dot_product = np.dot(embeddings_sample, embeddings_sample.T)\n",
    "dot_product.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac287514-b2d2-4c2c-8250-cbaeec97d73b",
   "metadata": {},
   "source": [
    "Given the above output, the dot product matrix has a shape of (1000, 1000), meaning we have similarity scores between all pairs of prompts in our sample of 1000.\n",
    "\n",
    "Next, let's choose a prompt and find the top 5 most similar prompts based on the computed similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1a18d-6b3b-4f28-8ca3-24ea8b1689d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt_idx = 23\n",
    "source_prompt = df.iloc[source_prompt_idx].prompt\n",
    "source_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a72a91-5a4c-447c-9489-4f7b4fa8844b",
   "metadata": {},
   "source": [
    "The prompt we're using as a reference (index 23) is:\n",
    "\n",
    "`'Write me a function to lazily compute a Fibonacci sequence in Clojure.'`\n",
    "\n",
    "Now let's find the top 5 most similar prompts to this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c8b1b8-9279-4c4f-87f5-f01f5f154822",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "similar_promts_idx = np.argsort(dot_product[source_prompt_idx])[-top_k:][::-1]\n",
    "similar_promts = df.iloc[similar_promts_idx].prompt\n",
    "similar_promts.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ce403-0390-44b5-9747-1f84f124c681",
   "metadata": {},
   "source": [
    "As you can see, these prompts are closely related to programming tasks, many of them dealing with functions and computations. This shows how embeddings can group semantically similar questions together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17908262-6a3f-4867-8c05-49cf5704954e",
   "metadata": {},
   "source": [
    "You also have the embeddings for responses from models a (`./chatbot-arena-model_a_response-embeddings.npy`) and model b (`./chatbot-arena-model_b_response-embeddings.npy`) respectively to explore. (These are the embeddings from columns `model_a_response` and `model_b_response` which can be created by extracting the second half of columns `conversation_a` and `conversation_b` respectively (as explained in the previous section)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae89944-fffb-4dd4-a33f-5f7286d3d669",
   "metadata": {},
   "source": [
    "Potential follow-up questions you can explore: \n",
    "- Can you identify clusters of similar topics within the dataset?\n",
    "- How do different models perform on similar prompts?\n",
    "- Can you find examples where semantically similar prompts result in different outcomes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39856510-10f6-4060-b837-013fbbeb5544",
   "metadata": {},
   "source": [
    "### Topic Modeling and Hardness Score Data\n",
    "\n",
    "Now, let's explore the second auxiliary dataset `./chatbot-arena-gpt3-scores.jsonl.gz`, which contains valuable information for later modeling tasks. \n",
    "\n",
    "For each prompt, there are 3 responses, as GPT-3.5 is probabilistic. This means we get multiple labels for a single prompt, similar to how real-world datasets can have multiple annotations.\n",
    "\n",
    "Let's start by loading and inspecting the first 5 rows of this dataset to understand its structure.\n",
    "\n",
    "**_Warning_: This data can be messy! This is intentionally not cleaned up for you to resemble real-world data. You are responsible for figuring out the irregularities and cleaning it up. The following cells demonstrate the example of messy data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9022d114-d745-496e-bbcc-0586f5f550b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1780b-4252-4ee7-85ac-a8670ce6d13c",
   "metadata": {},
   "source": [
    "**Understand the Structure**\n",
    "How are the topic modeling and hardness scores structured across the dataset?\n",
    "- Look at the columns `topic_modeling_1`, `score_reason_1`, `score_value_1`, etc.\n",
    "- How consistent are the values across the rows? Do they follow a pattern? Are there any irregularities or unexpected values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac487283-c64e-45e0-a215-b3494042a8d8",
   "metadata": {},
   "source": [
    "**Data Cleaning**\n",
    "The following are some of the things (many) you should explore to clean your data:\n",
    "- Are there any missing values?\n",
    "- The data in the `openai_scores_raw_choices_nested` field appears messy. It contains a list of nested dictionaries that might need to be flattened for easier analysis. Consider these points:\n",
    "    - Do you need all the nested data? Perhaps you only need the final hardness scores and topic modeling results for your analysis.\n",
    "    - One way to deal with the nested openai_scores_raw_choices_nested field is to flatten it into more readable columns if you find you need the nested data.\n",
    "- Sometimes, there could be repeated entries in the dataset. Make sure that each question (question_id) and its corresponding responses are unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f388a828-059c-47fc-a63b-5522e1ee0b43",
   "metadata": {},
   "source": [
    "Let’s begin by checking for missing values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0999b9-1e01-4422-be03-9ccb160872e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa314486-3380-4c66-8bc1-184c92cb237c",
   "metadata": {},
   "source": [
    "What are some strategies for handling missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb936ab-4d91-4913-abab-b6975a86a883",
   "metadata": {},
   "source": [
    "Messy data is not only about missing values, it can also be about inconsistent formatting. Let's check for inconsistencies in the `score_value_1` column of the dataset. The `score_value_1` field should contain numerical values representing hardness scores. However, sometimes data can be messy, and we might encounter values that are not in the expected format (e.g., lists instead of single numbers).\n",
    "\n",
    "The code below demonstrates how to identify rows where `score_value_1` is incorrectly formatted as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f073fdd-f625-490b-829e-1f4a97449456",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_and_hardness[\"score_value_1\"][\n",
    "    topic_and_hardness[\"score_value_1\"].apply(lambda x: isinstance(x, list))\n",
    "].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1779a36b-9613-4223-b480-c9af74bac103",
   "metadata": {},
   "source": [
    "Why do you think `score_value_1` contains lists instead of single values? What might have caused this? What steps would you take to fix this issue so that `score_value_1` contains only numeric values? Are there any other columns that might have similar issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed014e7-fa5f-4b8e-8720-a4453cc7158a",
   "metadata": {},
   "source": [
    "Once you clean the dataset, you can start your analysis by think about the following questions:\n",
    "- Do models perform differently based on hardness scores?\n",
    "- What are the most common topics in the dataset?\n",
    "- Which topics tend to have higher hardness scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc3be19-44db-4816-b8eb-96e26eb176fc",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "You've now explored various aspects of the dataset, including prompt lengths, response lengths, and embeddings, as well as handling messy data. As you proceed, it's important to remember the key **EDA requirements** for this milestone:\n",
    "\n",
    "1. **Ranking of the model based on their win rate or ELO ratings**\n",
    "2. **Distribution of the prompt and response length** \n",
    "4. **Hardness score distribution and its correlation with the models** \n",
    "5. **Open-ended: Visualize the \"variance\" in model performance**\n",
    "6. **Open-ended: Explore the prompt topics in the dataset (topic modeling)**\n",
    "\n",
    "### What Have You Discovered?\n",
    "\n",
    "Now that you've completed the initial exploration through this notebook, think about the following:\n",
    "\n",
    "- **Which insights surprised you?**  \n",
    "- **What further questions would you ask based on the patterns you've seen?**  \n",
    "- **Which analyses are still missing or could be extended?**\n",
    "\n",
    "This notebook provides a foundation for your EDA, but the real insights will come from the questions you ask and the deeper analyses you conduct. Use this notebook as a starting point to explore relationships in the data that will help inform your understanding of chatbot performance. By formulating and answering your questions as a team, you'll gain a richer understanding of the dataset and get ready for the modeling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
